{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J9ePsuUqIE2"
   },
   "source": [
    "## Проблемы исчезновения / взрывного роста градиентов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9JyCfHTqIE_"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j6a5TjcGqIE_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1717514136505,
     "user_tz": -180,
     "elapsed": 10769,
     "user": {
      "displayName": "Максим Степанович",
      "userId": "06303425006651651655"
     }
    },
    "outputId": "84ff69f0-7171-4b3b-9d86-72bb9a992d90",
    "ExecuteTime": {
     "end_time": "2024-06-06T19:14:41.089681Z",
     "start_time": "2024-06-06T19:14:35.940432Z"
    }
   },
   "source": [
    "\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDxnRgG6qIFA"
   },
   "source": [
    "Как мы обсуждали ранее, алгоритм обратного распространения работает, переходя от выходного слоя к входному слою, распространяя градиент ошибки по пути. После того как алгоритм вычислил градиент функции стоимости для каждого параметра в сети, он использует эти градиенты для обновления каждого параметра с шагом градиентного спуска.\n",
    "К сожалению, градиенты часто становятся все меньше и меньше по мере продвижения алгоритма к нижним уровням. В результате обновление Gradient Descent практически не изменяет вес соединений нижних уровней, и обучение никогда не приводит к хорошему решению. Мы называем это проблемой исчезающих градиентов . В некоторых случаях может произойти обратное: градиенты могут увеличиваться и увеличиваться до тех пор, пока слои не получат безумно большие обновления веса, а алгоритм не будет расходиться. Это проблема взрывающихся градиентов , которая возникает в рекуррентных нейронных сетях. В целом, глубокие нейронные сети страдают от нестабильных градиентов; разные слои могут учиться на разных скоростях.\n",
    "Это неудачное поведение наблюдалось эмпирически давно, и это было одной из причин, по которой глубокие нейронные сети были в основном заброшены в начале 2000-х годов. Неясно, что послужило причиной нестабильности градиентов при обучении DNN, но в 2010 paper https://homl.info/47 года Ксавье Глорот и Йошуа Бенджо был пролит свет. Авторы обнаружили несколько решений, в том числе сочетание популярной функции активации логистической сигмоиды и техники инициализации веса, которая была наиболее популярной в то время (то есть нормальное распределение со средним значением 0 и стандартным отклонением 1). Вкратце, они показали, что с этой функцией активации и этой схемой инициализации дисперсия выходов каждого слоя намного больше, чем дисперсия его входов. Продвигаясь вперед в сети, дисперсия продолжает увеличиваться после каждого слоя, пока функция активации не насыщается на верхних слоях. Это насыщение фактически усугубляется тем фактом, что логистическая функция имеет среднее значение 0,5, а не 0 (гиперболическая касательная функция имеет среднее значение 0 и ведет себя немного лучше, чем логистическая функция в глубоких сетях).\n",
    "Рассматривая функцию логистической активации (см. Рис. 11-1), вы можете видеть, что, когда входы становятся большими (отрицательными или положительными), функция насыщается на 0 или 1, а производная очень близка к 0. Таким образом, когда обратное распространение начинается у него практически нет градиента для распространения по сети; и то, что существует небольшой градиент, продолжает разбавляться по мере того, как обратное распространение распространяется вниз через верхние слои, поэтому для нижних слоев действительно ничего не остается."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q7KUk0mcqIFA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1717514177282,
     "user_tz": -180,
     "elapsed": 266,
     "user": {
      "displayName": "Максим Степанович",
      "userId": "06303425006651651655"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-06-06T19:14:41.095609Z",
     "start_time": "2024-06-06T19:14:41.091686Z"
    }
   },
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "H4Kltt8EqIFB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1717514178843,
     "user_tz": -180,
     "elapsed": 1290,
     "user": {
      "displayName": "Максим Степанович",
      "userId": "06303425006651651655"
     }
    },
    "outputId": "cb33316a-41fb-40f5-b4c2-d6b2da529c05",
    "ExecuteTime": {
     "end_time": "2024-06-06T19:14:41.724792Z",
     "start_time": "2024-06-06T19:14:41.096614Z"
    }
   },
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "save_fig(\"sigmoid_saturation_plot\")\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure sigmoid_saturation_plot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcxbXbzKqIFB"
   },
   "source": [
    "## Glorot и He инициализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgtMFuDjqIFB"
   },
   "source": [
    "В своей работе Glorot и Bengio предлагают способ значительно облегчить проблему нестабильных градиентов. Они указывают на то, что нам нужно, чтобы сигнал работал правильно в обоих направлениях: в прямом направлении при прогнозировании и в обратном направлении при обратном распространении градиентов. Мы не хотим, чтобы сигнал угас, и при этом мы не хотим, чтобы он взрывался и насыщался. Для получения требуемого уровня сигнала, авторы утверждают, что нам нужно чтобы дисперсия выходов каждого слоя должна быть равной дисперсии его входов, и нам нужно градиент, которые имеют равные дисперсии до и после прохождения через слой в обратном направлении. Это на самом деле не представляется возможным, чтобы это гарантировать так как если слой не имеет равное количество входов и нейронов, но Glorot и Bengio предложили хороший компромисс, который доказал свою работу очень хорошо на практике: веса соединений каждого слоя должны быть инициализированы случайным образом, как описано в уравнении 11-1 , где $fan_{avg} =\n",
    "(fan_{in} + fan_{out})/2$. Эта стратегия инициализации называется инициализацией Ксавье или инициализация Glorot, после первого автора статьи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pux9RRClqIFB"
   },
   "source": [
    "**Equation 11-1: Xavier initialization (when using the logistic activation function)**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "& \\text{Normal distribution with mean 0 and standard deviation }\n",
    "\\sigma = \\sqrt{\\dfrac{2}{n_\\text{inputs} + n_\\text{outputs}}}\\\\\n",
    "& \\text{Or a uniform distribution between -r and +r, with }\n",
    "r = \\sqrt{\\dfrac{6}{n_\\text{inputs} + n_\\text{outputs}}}\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9QqkJ5jqIFB"
   },
   "source": [
    "Если вы замените $fan_{avg}$ на $fan_{in}$ в уравнении 11-1, вы получите стратегию инициализации, предложенную Yann LeCun в 1990-х годах. Он назвал это инициализацией LeCun . Женевьева Орр и Клаус-Роберт Мюллер даже рекомендовали это в своей книге «Нейронные сети: хитрости торговли» (Springer) 1998 года. Инициализация LeCun эквивалентна инициализации Glorot, когда $fan_{in} = fan_{out}$. Исследователям потребовалось более десяти лет, чтобы понять, насколько важен этот трюк. Использование инициализации Glorot может значительно ускорить обучение, и это один из приемов, которые привели к успеху глубокого обучения.\n",
    "В некоторых статьях представлены аналогичные стратегии для разных функций активации. Эти стратегии отличаются только масштабом дисперсии и используют ли они $fan_{avg}$ на $fan_{in}$, как показано в таблице 11-1  (для равномерного распределения, просто вычислить $r = \\sqrt{3 \\sigma^2}$. Стратегия инициализации для функции активации ReLU (и ее варианты, включая активацию ELU, описанную ниже) иногда называется инициализацией He в честь имени первого автора статьи. Функция активации SELU будет объяснена позже. Она может быть использована с инициализацией LeCun (желательно с нормальным распределением, как мы увидим)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNfsnS7NqIFB"
   },
   "source": [
    "**Table 11-1: Initialization parameters for each type of activation function**\n",
    "\n",
    "* Logistic uniform: $ r = \\sqrt{\\dfrac{6}{n_\\text{inputs} + n_\\text{outputs}}} $\n",
    "* Logistic normal: $ \\sigma = \\sqrt{\\dfrac{2}{n_\\text{inputs} + n_\\text{outputs}}} $\n",
    "* Hyperbolic tangent uniform: $ r = 4 \\sqrt{\\dfrac{6}{n_\\text{inputs} + n_\\text{outputs}}} $\n",
    "* Hyperbolic tangent normal: $ \\sigma = 4 \\sqrt{\\dfrac{2}{n_\\text{inputs} + n_\\text{outputs}}} $\n",
    "* ReLU (and its variants) uniform: $ r = \\sqrt{2} \\sqrt{\\dfrac{6}{n_\\text{inputs} + n_\\text{outputs}}} $\n",
    "* ReLU (and its variants) normal: $ \\sigma = \\sqrt{2} \\sqrt{\\dfrac{2}{n_\\text{inputs} + n_\\text{outputs}}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zBJtnWaqIFC"
   },
   "source": [
    "<img src = \"tab11_1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kCJUONDqIFC"
   },
   "source": [
    "По умолчанию Keras использует инициализацию Glorot с равномерным распределением. При создании слоя вы можете изменить это на инициализацию He, установив ```python kernel_initializer = \"he_uniform\"``` или ```python kernel_initializer = \"he_normal\"``` следующим образом:\n",
    "```python keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\") ```\n",
    "Если вы хотите его инициализацию с равномерным распределением, но на основе $fan_{avg}$ , а $fan_{in}$ в , вы можете использовать ```python VarianceScaling``` инициализатора, например так:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUfrkMy6qIFC"
   },
   "source": [
    "```python\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "                                                 distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7wVyIWoOqIFC",
    "outputId": "5d0565cd-ef17-4312-f9e2-e46d9cd00540",
    "ExecuteTime": {
     "end_time": "2024-06-06T19:14:41.764289Z",
     "start_time": "2024-06-06T19:14:41.726803Z"
    }
   },
   "source": [
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'IdentityInitializer',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'OrthogonalInitializer',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xJ9pGlE-qIFC",
    "outputId": "aa74acb8-160a-4fb9-c8a3-e4eccca12efa",
    "ExecuteTime": {
     "end_time": "2024-06-06T19:14:41.771784Z",
     "start_time": "2024-06-06T19:14:41.765295Z"
    }
   },
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Dense name=dense, built=False>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "id4SFnldqIFC",
    "outputId": "d2f355af-0609-4780-cc7d-ac3793837f75",
    "ExecuteTime": {
     "end_time": "2024-06-06T19:14:41.778234Z",
     "start_time": "2024-06-06T19:14:41.772790Z"
    }
   },
   "source": [
    "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "                                          distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Dense name=dense_1, built=False>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvSW2mcpqIFC"
   },
   "source": [
    "## Ненасыщенные функции активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff39AFMNqIFC"
   },
   "source": [
    "Одним из выводов, сделанных Глоротом и Бенжио в статье 2010 года, было то, что проблемы с нестабильными градиентами частично были связаны с плохим выбором функции активации. До тех пор большинство людей полагали, что, если Мать-Природа решила использовать функции активации сигмовидной железы в биологических нейронах, то сигмоида должна быть отличным выбором. Но оказывается, что другие функции активации ведут себя намного лучше в глубоких нейронных сетях - в частности, функция активации ReLU, главным образом потому, что она не насыщает положительные значения (и потому, что она быстро вычисляется).\n",
    "К сожалению, функция активации ReLU не идеальна. Он страдает от проблемы, известной как умирающие ReLU : во время обучения некоторые нейроны эффективно «умирают», что означает, что они перестают выводить что-либо кроме 0. В некоторых случаях вы можете обнаружить, что половина нейронов вашей сети мертва, особенно если вы использовали большая скорость обучения. Нейрон умирает, когда его веса изменяются таким образом, что взвешенная сумма его входов является отрицательной для всех экземпляров в обучающем наборе. Когда это происходит, он просто продолжает выводить нули, и Gradient Descent больше не влияет на него, потому что градиент функции ReLU равен нулю, когда ее вход отрицателен.\n",
    "Чтобы решить эту проблему, вы можете использовать вариант функции LeakyReLU, такой как ReLU с утечкой . Эта функция определяется как LeakyReLU $α( z ) = max(αz , z)$ (см. Рис. 11-2  ). Гиперпараметр α определяет, насколько «утечка» функции: это угол наклона функции для z <0 и обычно устанавливается на 0,01. Этот небольшой уклон гарантирует, что протекающие ReLU никогда не погибнут; они могут впасть в длительную кому, но у них есть шанс в конце концов проснуться. В 2015 году сравнивалось несколько вариантов функции активации ReLU, и один из ее выводов заключался в том, что неплотные варианты всегда превосходили строгую функцию активации ReLU. Фактически, установка α = 0,2 (огромная утечка), казалось, привела к лучшей производительности, чем α = 0,01 (небольшая утечка). В документе также оценивалась рандомизированная утечка ReLU (RReLU), где α выбирается случайным образом в заданном диапазоне во время обучения и фиксируется на среднем значении во время тестирования. RReLU также работал довольно хорошо и, похоже, выступал в качестве регуляризатора (снижая риск переобучения). Наконец, в статье оценивается параметрическая утечка ReLU (PReLU), где α разрешается изучать во время обучения (вместо того, чтобы быть гиперпараметром, он становится параметром, который может быть изменен путем обратного распространения, как любой другой параметр). Сообщалось, что PReLU сильно превосходит ReLU в наборах данных с большими изображениями, но для небольших наборов данных он рискует переобучить тренировочный набор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Amgc_olpqIFC"
   },
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "64qBd3ceqIFC",
    "ExecuteTime": {
     "end_time": "2024-06-06T19:14:41.785056Z",
     "start_time": "2024-06-06T19:14:41.779240Z"
    }
   },
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yNF_SaV5qIFC",
    "outputId": "ec2749ec-f7b3-4ce3-b8ac-bf3d7dc1013b",
    "ExecuteTime": {
     "end_time": "2024-06-06T19:14:42.224153Z",
     "start_time": "2024-06-06T19:14:41.787070Z"
    }
   },
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"leaky_relu_plot\")\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure leaky_relu_plot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U4mrboyyqIFC",
    "outputId": "3b9c9166-0a2c-4037-e00b-18ec627d3af7",
    "ExecuteTime": {
     "end_time": "2024-06-06T19:14:42.231471Z",
     "start_time": "2024-06-06T19:14:42.225191Z"
    }
   },
   "source": [
    "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deserialize',\n",
       " 'elu',\n",
       " 'exponential',\n",
       " 'gelu',\n",
       " 'get',\n",
       " 'hard_sigmoid',\n",
       " 'hard_silu',\n",
       " 'hard_swish',\n",
       " 'leaky_relu',\n",
       " 'linear',\n",
       " 'log_softmax',\n",
       " 'mish',\n",
       " 'relu',\n",
       " 'relu6',\n",
       " 'selu',\n",
       " 'serialize',\n",
       " 'sigmoid',\n",
       " 'silu',\n",
       " 'softmax',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'swish',\n",
       " 'tanh']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e65UD61nqIFC",
    "outputId": "d86cba92-fa16-410a-993b-200ffcdb5608",
    "ExecuteTime": {
     "end_time": "2024-06-06T19:14:42.240878Z",
     "start_time": "2024-06-06T19:14:42.232476Z"
    }
   },
   "source": [
    "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slJ60CwuqIFD"
   },
   "source": [
    "Let's train a neural network on Fashion MNIST using the Leaky ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blNDbfb5qIFD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1717514187805,
     "user_tz": -180,
     "elapsed": 1451,
     "user": {
      "displayName": "Максим Степанович",
      "userId": "06303425006651651655"
     }
    },
    "outputId": "7562c681-d9ef-4fa2-c762-057d98c32c95",
    "ExecuteTime": {
     "end_time": "2024-06-06T19:14:42.703141Z",
     "start_time": "2024-06-06T19:14:42.242885Z"
    }
   },
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YLq--8YNqIFD",
    "ExecuteTime": {
     "end_time": "2024-06-06T19:14:42.795825Z",
     "start_time": "2024-06-06T19:14:42.704146Z"
    }
   },
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilyar\\teachmeskills\\TMSds\\venv\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cl_4zkkFqIFD",
    "outputId": "55ec44e4-8bbc-483f-a10b-645a4049c467",
    "ExecuteTime": {
     "end_time": "2024-06-06T19:14:44.079702Z",
     "start_time": "2024-06-06T19:14:42.798832Z"
    }
   },
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument(s) not recognized: {'lr': 0.001}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse_categorical_crossentropy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m----> 2\u001B[0m               optimizer\u001B[38;5;241m=\u001B[39m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSGD\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-3\u001B[39;49m\u001B[43m)\u001B[49m,\n\u001B[0;32m      3\u001B[0m               metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[1;32m~\\teachmeskills\\TMSds\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\sgd.py:60\u001B[0m, in \u001B[0;36mSGD.__init__\u001B[1;34m(self, learning_rate, momentum, nesterov, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     44\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     45\u001B[0m     learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m     59\u001B[0m ):\n\u001B[1;32m---> 60\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclipnorm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclipnorm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclipvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclipvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m        \u001B[49m\u001B[43mglobal_clipnorm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mglobal_clipnorm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_ema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_ema\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m        \u001B[49m\u001B[43mema_momentum\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mema_momentum\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m        \u001B[49m\u001B[43mema_overwrite_frequency\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mema_overwrite_frequency\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloss_scale_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_scale_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgradient_accumulation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgradient_accumulation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(momentum, \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m momentum \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m momentum \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m     75\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`momentum` must be a float between [0, 1].\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\teachmeskills\\TMSds\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:22\u001B[0m, in \u001B[0;36mTFOptimizer.__init__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 22\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_distribution_strategy \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mget_strategy()\n",
      "File \u001B[1;32m~\\teachmeskills\\TMSds\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:37\u001B[0m, in \u001B[0;36mBaseOptimizer.__init__\u001B[1;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001B[0m\n\u001B[0;32m     33\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m     34\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mArgument `decay` is no longer supported and will be ignored.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     35\u001B[0m     )\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs:\n\u001B[1;32m---> 37\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mArgument(s) not recognized: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkwargs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     40\u001B[0m     name \u001B[38;5;241m=\u001B[39m auto_name(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Argument(s) not recognized: {'lr': 0.001}"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "0_rvq8MIqIFD",
    "outputId": "681004f5-074b-4e0e-a701-75ca67230994",
    "ExecuteTime": {
     "end_time": "2024-06-06T19:14:44.081709Z",
     "start_time": "2024-06-06T19:14:44.081709Z"
    }
   },
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RUbTYF5qIFD"
   },
   "source": [
    "Посмотрим как влияет на качество обучения RPLU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "leHiqkP_qIFD"
   },
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4w1WOMtCqIFD",
    "outputId": "ae33e7db-127a-4b8f-acbd-fecaa4b35b02"
   },
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8yXz5EcAqIFD",
    "outputId": "6c7f50a8-f122-449a-8edc-49fcd9fcb5ec"
   },
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCRUCTQxqIFD"
   },
   "source": [
    "И последнее, но не менее важное: статья Djork-Arné Clevert et al. предложил новую функцию активации, названную экспоненциальной линейной единицей (ELU), которая превзошла все варианты ReLU в экспериментах авторов: время обучения было сокращено, и нейронная сеть работала лучше на тестовом наборе. На рисунке 11-3  представлена функция графика, а в уравнении 11-2 показано ее определение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bhj6F0tfqIFD"
   },
   "source": [
    "**Equation 11-2: ELU activation function**\n",
    "\n",
    "$\n",
    "\\operatorname{ELU}_\\alpha(z) =\n",
    "\\begin{cases}\n",
    "\\alpha(\\exp(z) - 1) & \\text{if } z < 0\\\\\n",
    "z & if z \\ge 0\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYxh03DsqIFD"
   },
   "source": [
    "Функция активации ELU очень похожа на функцию ReLU, с некоторыми существенными отличиями:\n",
    "*\tОн принимает отрицательные значения, когда z <0, что позволяет устройству иметь средний выходной сигнал ближе к 0 и помогает решить проблему исчезающих градиентов. Гиперпараметр α определяет значение, к которому приближается функция ELU, когда z является большим отрицательным числом. Обычно он равен 1, но вы можете настроить его, как любой другой гиперпараметр.\n",
    "*\tОн имеет ненулевой градиент для z <0, что позволяет избежать проблемы мертвых нейронов.\n",
    "*\tЕсли α равно 1, то функция является гладкой везде, в том числе около z = 0, что помогает ускорить градиентный спуск, поскольку она не сильно отскакивает влево и вправо от z = 0.\n",
    "Основным недостатком функции активации ELU является то, что она медленнее вычисляется, чем функция ReLU и ее варианты (из-за использования экспоненциальной функции). Более высокая скорость сходимости во время обучения компенсирует это медленное вычисление, но, тем не менее, во время тестирования сеть ELU будет работать медленнее, чем сеть ReLU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAErPDYRqIFD"
   },
   "source": [
    "### ELU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8oiMfeh-qIFD"
   },
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ystOr__LqIFD",
    "outputId": "e0591d44-0c06-4d87-ea3e-32b98b626a62"
   },
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"elu_plot\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIrq7Dv4qIFH"
   },
   "source": [
    "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oXLjpkcgqIFH",
    "outputId": "c6300d0a-3c55-41a3-c280-332477077b97"
   },
   "source": [
    "keras.layers.Dense(10, activation=\"elu\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd1MpmbHqIFH"
   },
   "source": [
    "Затем в 2017[great paper](https://arxiv.org/pdf/1706.02515.pdf) Гюнтер Кламбауэр и соавт. представил функцию активации Scaled ELU (SELU): как следует из названия, это масштабированный вариант функции активации ELU. Авторы показали, что если вы строите нейронную сеть, состоящую исключительно из стека плотных слоев, и если все скрытые слои используют функцию активации SELU, то сеть будет самостоятельно нормализоваться: выходные данные каждого слоя будут иметь тенденцию сохранять среднее значение 0 и стандартное отклонение 1 во время тренировки, что решает проблему исчезающих / взрывных градиентов. В результате функция активации SELU часто значительно превосходит другие функции активации для таких нейронных сетей (особенно глубоких). Однако есть несколько условий для самонормализации (математическое обоснование см. В статье):\n",
    "•\tВходные характеристики должны быть стандартизированы (среднее значение 0 и стандартное отклонение 1).\n",
    "•\tВес каждого скрытого слоя должен быть инициализирован с помощью обычной инициализации LeCun. В Keras это означает установку kernel_initializer = \"lecun_normal\" .\n",
    "•\tАрхитектура сети должна быть последовательной. К сожалению, если вы попытаетесь использовать SELU в непоследовательных архитектурах, таких как рекуррентные сети или сети с пропущенными соединениями (т. Е. Соединения, которые пропускают слои, например, в сетях Wide & Deep), самонормализация не гарантируется Таким образом, SELU не обязательно будет превосходить другие функции активации.\n",
    "•\tВ статье гарантируется самонормализация только в том случае, если все слои плотные, но некоторые исследователи отмечают, что функция активации SELU может улучшить производительность и в сверточных нейронных сетях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tci3z_JGqIFI"
   },
   "source": [
    "### SELU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jDrr0qj-qIFI"
   },
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "# (see equation 14 in the paper):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZozcXHTCqIFI"
   },
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cneoo9vAqIFI",
    "outputId": "926d79f5-d15e-47fe-81fc-3cfdf7daf61c"
   },
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"selu_plot\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR_DoFzAqIFI"
   },
   "source": [
    "By default, the SELU hyperparameters (`scale` and `alpha`) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DKOqmG0UqIFI",
    "outputId": "252c9440-bfdb-4ef4-88dd-878981807d77"
   },
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # standardized inputs\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03VNELpVqIFI"
   },
   "source": [
    "Using SELU is easy:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GaQ_H2cpqIFI",
    "outputId": "a89692c4-90c3-4c74-cee0-1da1dae485aa"
   },
   "source": [
    "keras.layers.Dense(10, activation=\"selu\",\n",
    "                   kernel_initializer=\"lecun_normal\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w84WdqgLqIFI"
   },
   "source": [
    "Let's create a neural net for Fashion MNIST with 100 hidden layers, using the SELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UaCNuDgDqIFI"
   },
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a3GFltfDqIFI"
   },
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qsFIS4tXqIFI",
    "outputId": "da15ce4f-7ad1-4037-e12f-d60be09bd7bd"
   },
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NcV5nbAqIFI"
   },
   "source": [
    "Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TYmovEFJqIFI"
   },
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "__ox-F1-qIFJ",
    "outputId": "56e5c7fb-8869-4c1f-d0d6-b490e330648e"
   },
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4n0n8TqqIFJ"
   },
   "source": [
    "Now look at what happens if we try to use the ReLU activation function instead:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cGOQfrDVqIFJ"
   },
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4CLDX0-LqIFJ"
   },
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OSdxGHX_qIFJ",
    "outputId": "fb6de29c-b0fe-49ae-84fd-fa76c9582bf0"
   },
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LVMyhA8NqIFJ",
    "outputId": "09d6993a-95db-4e6b-9021-4f2fdff3f744"
   },
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0ARiUZDqIFJ"
   },
   "source": [
    "Итак, какую функцию активации вы должны использовать для скрытых слоев ваших глубоких нейронных сетей? Хотя ваш разброс будет варьироваться, в общем SELU> ELU> неплотный ReLU (и его варианты)> ReLU> tanh> logistic. Если архитектура сети предотвращает ее самонормализацию, то ELU может работать лучше, чем SELU (поскольку SELU не является плавным при z = 0). Если вас сильно волнует задержка во время выполнения, вы можете предпочесть негерметичный ReLU. Если вы не хотите настраивать еще один гиперпараметр, вы можете использовать значения α по умолчанию, используемые Keras (например, 0,3 для неплотного ReLU). Если у вас есть свободное время и вычислительные мощности, вы можете использовать перекрестную проверку для оценки других функций активации, таких как RReLU, если ваша сеть перегружена, или PReLU, если у вас огромный тренировочный набор. Тем не менее, поскольку ReLU является наиболее часто используемой функцией активации (на данный момент), многие библиотеки и аппаратные ускорители обеспечивают специфичные для ReLU оптимизации; поэтому, если скорость является вашим приоритетом, ReLU все еще может быть лучшим выбором.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-corjzrqIFJ"
   },
   "source": [
    "# Пакетная нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN5aVTzwqIFJ"
   },
   "source": [
    "Хотя использование инициализации He вместе с ELU (или любым вариантом ReLU) может значительно снизить опасность проблем исчезающего/взрывного градиента в начале обучения, это не гарантирует, что они не вернутся во время обучения.\n",
    "В 2015 Сергей Иоффе и Кристиан Сегеди предложили метод, называемый нормализацией партии (BN), который решает эти проблемы. Техника состоит в добавлении операции в модель непосредственно перед или после функции активации каждого скрытого слоя. Эта операция просто центрирует и нормализует каждый вход, затем масштабирует и сдвигает результат, используя два новых вектора параметров на слой: один для масштабирования, другой для сдвига. Другими словами, операция позволяет модели узнать оптимальный масштаб и среднее значение каждого из входов слоя. Во многих случаях, если вы добавляете слой BN в качестве самого первого слоя вашей нейронной сети, вам не нужно стандартизировать свой тренировочный набор (например, используя StandardScaler ); слой BN сделает это за вас (ну, примерно, так как он просматривает только один пакет за раз, и он также может масштабировать и сдвигать каждую входную функцию).\n",
    "Для того, чтобы центрировать и нормализовать входы, алгоритм должен оценить среднее значение и стандартное отклонение каждого входа. Это достигается путем оценки среднего и стандартного отклонения входного сигнала по текущей мини-партии (отсюда и название «нормализация партии»). Вся операция суммируется шаг за шагом в уравнении 11-3 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAr24ehfqIFJ"
   },
   "source": [
    "**Equation 11-3: Batch Normalization algorithm**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "1.\\quad & \\mathbf{\\mu}_B = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{\\mathbf{x}^{(i)}}\\\\\n",
    "2.\\quad & {\\mathbf{\\sigma}_B}^2 = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{(\\mathbf{x}^{(i)} - \\mathbf{\\mu}_B)^2}\\\\\n",
    "3.\\quad & \\hat{\\mathbf{x}}^{(i)} = \\dfrac{\\mathbf{x}^{(i)} - \\mathbf{\\mu}_B}{\\sqrt{{\\mathbf{\\sigma}_B}^2 + \\epsilon}}\\\\\n",
    "4.\\quad & \\mathbf{z}^{(i)} = \\gamma \\otimes \\hat{\\mathbf{x}}^{(i)} + \\beta\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCXJyOWwqIFJ"
   },
   "source": [
    "В этом алгоритме:\n",
    "*\t${\\mu}_B$ - вектор средних значений ввода, оцениваемый по всей мини-партии B (он содержит одно среднее значение для каждого входа).\n",
    "*\t${\\sigma}_B$ - вектор стандартных отклонений на входе, также оцениваемый по всей мини-партии (он содержит одно стандартное отклонение на вход).\n",
    "*\t$m_B$ - количество экземпляров в мини-партии.\n",
    "*\t$\\hat{\\mathbf{x}}^{(i)}$ - вектор нулевых центрированных и нормализованных входных данных, например, i .\n",
    "*\t$\\gamma$ - вектор выходного параметра масштаба для слоя (он содержит один параметр масштаба на вход).\n",
    "*\t$\\otimes$ представляет поэлементное умножение (каждый вход умножается на соответствующий выходной масштабный параметр).\n",
    "*\t$\\beta$ - вектор параметров выходного смещения (смещения) для слоя (он содержит один параметр смещения на вход). Каждый вход смещен соответствующим параметром сдвига.\n",
    "*\t$\\epsilon$ - крошечное число, которое избегает деления на ноль (обычно $10^{–5}$ ). Это называется сглаживающим термином.\n",
    "*\t${z}^{(i)}$ - результат операции BN. Это измененная и измененная версия входных данных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0aephTwqIFJ"
   },
   "source": [
    "Таким образом, во время обучения BN стандартизирует свои входные данные, затем масштабирует и компенсирует их. Хорошо! Как насчет во время теста? Ну, это не так просто. Действительно, нам может потребоваться делать прогнозы для отдельных экземпляров, а не для пакетов экземпляров: в этом случае у нас не будет возможности вычислить среднее значение и стандартное отклонение для каждого входа. Более того, даже если у нас есть пакет экземпляров, он может быть слишком маленьким или экземпляры могут быть независимыми и одинаково распределенными, поэтому вычисление статистики по экземплярам пакета будет ненадежным. Одним из решений может быть ожидание до конца обучения, а затем выполнение всего обучающего набора через нейронную сеть и вычисление среднего и стандартного отклонения каждого входа уровня BN. Эти «окончательные» средства ввода и стандартные отклонения могут затем использоваться вместо средств пакетного ввода и стандартных отклонений при прогнозировании. Тем не менее, большинство реализаций Пакетной нормализации оценивают эти окончательные статистические данные во время обучения, используя скользящую среднюю из средств ввода слоя и стандартные отклонения. Это то, что делает Keras автоматически, когда вы используете слой BatchNormalization . Подводя итог, можно узнать четыре вектора параметров в каждом нормированном на пакет уровне: γ (вектор выходного масштаба) и β (вектор выходного смещения) изучаются посредством регулярного обратного распространения, а μ (конечный вектор среднего входного значения) и σ ( конечный входной вектор стандартного отклонения) оценивается с использованием экспоненциальной скользящей средней. Обратите внимание, что μ и σ оцениваются во время обучения, но они используются только после обучения (для замены средств пакетного ввода и стандартных отклонений в уравнении 11-3 ).\n",
    "Иоффе и Сегеди продемонстрировали, что пакетная нормализация значительно улучшила все глубокие нейронные сети, с которыми они экспериментировали, что привело к значительному улучшению задачи классификации ImageNet (ImageNet - это большая база данных изображений, классифицированных по многим классам, которые обычно используются для оценки систем компьютерного зрения). Проблема исчезающих градиентов была сильно уменьшена до такой степени, что они могли использовать насыщающие функции активации, такие как танх и даже логистическую функцию активации. Сети были также намного менее чувствительны к инициализации веса. Авторы смогли использовать гораздо более высокие темпы обучения, значительно ускоряя процесс обучения. В частности, они отмечают, что:\n",
    "Применительно к современной модели классификации изображений нормализация партии достигает той же точности с 14-кратным сокращением шагов обучения и значительно превосходит исходную модель. […] Используя ансамбль нормализованных по партиям сетей, мы улучшаем лучший опубликованный результат по классификации ImageNet: достигаем 4,9% ошибок проверки топ-5 (и 4,8% ошибок тестирования), превышая точность оценок людей.\n",
    "Наконец, как подарок, который продолжает дарить, нормализация партии действует как регуляризатор, уменьшая потребность в других методах регуляризации (таких как выпадение, описанное ниже в этой главе).\n",
    "Пакетная нормализация, однако, добавляет некоторую сложность модели (хотя она может устранить необходимость нормализации входных данных, как мы обсуждали ранее). Кроме того, существует штраф за время выполнения: нейронная сеть делает более медленные прогнозы из-за дополнительных вычислений, требуемых на каждом уровне. К счастью, после тренировки часто можно объединить слой BN с предыдущим, что позволяет избежать штрафа за время выполнения. Это делается путем обновления весов и смещений предыдущего уровня, чтобы он напрямую генерировал выходные данные соответствующего масштаба и смещения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5eJI84JqIFJ"
   },
   "source": [
    "Как и в большинстве случаев с Keras, реализация нормализации партии проста и интуитивно понятна. Просто добавьте слой BatchNormalization до или после функции активации каждого скрытого слоя и при необходимости добавьте слой BN, а также первый слой в вашей модели. Например, эта модель применяет BN после каждого скрытого слоя и в качестве первого слоя в модели (после выравнивания входных изображений):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0KU_rdvjqIFJ"
   },
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2o40N6e8qIFK",
    "outputId": "e29827bc-faf8-4327-dfce-06a2bcaf5a06"
   },
   "source": [
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nMHJxfWqIFK"
   },
   "source": [
    "Как видите, каждый уровень BN добавляет четыре параметра на вход: γ , β , µ и σ (например, первый уровень BN добавляет 3136 параметров, что составляет 4 × 784). Последние два параметра, μ и σ , являются скользящими средними; обратное распространение не влияет на них, поэтому Keras называет их «необучаемыми» 9 (если вы посчитаете общее количество параметров BN, 3136 + 1200 + 400 и разделите на 2, вы получите 2 368, то есть общее число -тренируемые параметры в этой модели).\n",
    "Давайте посмотрим на параметры первого слоя BN. Два обучаемых (путем обратного распространения), а два нет:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DU0zH3LqIFK"
   },
   "source": [
    "Теперь, когда вы создаете слой BN в Keras, он также создает две операции, которые будут вызываться Keras на каждой итерации во время обучения. Эти операции обновят скользящие средние. Поскольку мы используем бэкэнд TensorFlow, эти операции являются операциями TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nlX42lTGqIFK",
    "outputId": "042247fa-df0f-4101-b509-95f412937fee"
   },
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eSUxNu0FqIFK",
    "outputId": "3421ff80-f21e-4d19-f5ea-459077f4a5e2"
   },
   "source": [
    "bn1.updates"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmjsPyW4qIFK"
   },
   "source": [
    "Авторы статьи BN высказались за добавление слоев BN перед функциями активации, а не после (как мы только что сделали). Существует некоторая дискуссия по этому поводу, которая, кажется, является предпочтительной, зависит от задачи - вы также можете поэкспериментировать с этим, чтобы увидеть, какой вариант лучше всего работает с вашим набором данных. Чтобы добавить слои BN перед функциями активации, необходимо удалить функцию активации из скрытых слоев и добавить их как отдельные слои после слоев BN. Более того, поскольку слой нормализации партии включает в себя один параметр смещения на вход, вы можете удалить член смещения из предыдущего слоя (просто передайте use_bias = False при его создании):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ae60u-YWqIFK",
    "outputId": "e2b97f42-21b3-46dc-bb69-7b6f72636f51"
   },
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SdoWCZ8OqIFK",
    "outputId": "73bb1c12-4682-431b-abda-cd2686f11d94"
   },
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqh-Q5x-qIFK"
   },
   "source": [
    "Класс `BatchNormalization` имеет несколько гиперпараметров, которые можно настроить. Значения по умолчанию, как правило, будут хорошими, но иногда вам может понадобиться настроить импульс. Этот гиперпараметр используется слоем `BatchNormalization` при обновлении экспоненциальных скользящих средних; задается новое значение $\\hat V$ (т.е., новый вектор входных параметров или стандартных отклонений, вычисленных по текущей партии), слой обновляет скользящий средний $\\hat V$\n",
    "используя следующее уравнение:\n",
    "$$\\hat V ← \\hat V × momentum + V × (1 − momentum)$$\n",
    "Хорошее значение импульса обычно близко к 1; например, 0,9, 0,99 или 0,999 (вам нужно больше 9 с для больших наборов данных и меньших мини-пакетов).\n",
    "Другим важным гиперпараметром является ось : она определяет, какую ось следует нормализовать. По умолчанию он равен –1, что означает, что по умолчанию он нормализует последнюю ось (используя средние значения и стандартные отклонения, рассчитанные по другим осям). Когда входной пакет является 2D (т. е. Форма пакета [batch size, features]), это означает, что каждый входной объект будет нормализован на основе среднего значения и стандартного отклонения, рассчитанного для всех экземпляров в пакете. Например, первый уровень BN в предыдущем примере кода будет независимо нормализовать (и масштабировать и сдвигать) каждую из 784 входных параметров. Если мы переместим первый слой BN перед слоем Flatten, то входные пакеты будут 3D, с формой [batch size, height, width]; следовательно, слой BN будет вычислять 28 средних значений и 28 стандартных отклонений (1 на столбец пикселей, рассчитанный для всех экземпляров в пакете и для всех строк в столбце), и он будет нормализовать все пиксели в данном столбце с использованием того же среднего значения и стандартное отклонение. Также будет только 28 масштабных параметров и 28 параметров сдвига. Если вместо этого вы все еще хотите обрабатывать каждый из 784 пикселей независимо, то вам следует установить axis = [1, 2].\n",
    "Обратите внимание, что уровень BN не выполняет одинаковые вычисления во время тренировки и после тренировки: он использует статистику партии во время тренировки и «окончательную» статистику после тренировки (т. е. конечные значения скользящих средних). Давайте посмотрим на исходный код этого класса, чтобы увидеть, как это обрабатывается:\n",
    "```python\n",
    "class BatchNormalization(keras.layers.Layer):\n",
    "    [...]\n",
    "    def call(self, inputs, training=None):\n",
    "        [...]\n",
    "```\n",
    "Метод `call()` - он непосредственно выполняет вычисления; как видите, он имеет дополнительный обучающий аргумент, который по умолчанию имеет значение None , но метод `fit()` устанавливает его равным 1 во время обучения. Если вам когда-нибудь понадобится написать пользовательский слой, и он должен вести себя по-разному во время обучения и тестирования, добавьте обучающий аргумент в метод `call()` и используйте этот аргумент в методе, чтобы решить, что вычислять 10.\n",
    "Пакетная нормализация стала одним из наиболее часто используемых слоев в глубоких нейронных сетях, так что на диаграммах она часто опускается, так как предполагается, что BN добавляется после каждого слоя. Но недавняя статья Хонги Чжан и соавт. может изменить это предположение: используя новую методику инициализации веса с фиксированным обновлением (fixup), авторам удалось обучить очень глубокую нейронную сеть (10 000 слоев!) без BN, достигая современного уровня производительности при сложной классификации изображений задачи. Однако, так как это новейшее исследование, вы можете подождать дополнительных исследований, чтобы подтвердить этот вывод, прежде чем отказаться от нормализации партии."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b7-tWom4qIFK"
   },
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HVepg_5DqIFK",
    "outputId": "f4eaa273-fb7f-4f9f-8d26-147d8ff4a448"
   },
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WOwMH9hOqIFK",
    "outputId": "a3ba90b9-56d0-4c74-9990-bac5a4787438"
   },
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuCdFP-uqIFK"
   },
   "source": [
    "## Градиент отсечения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGDfr9IvqIFL"
   },
   "source": [
    "Другой популярный метод для смягчения проблемы взрывающихся градиентов состоит в том, чтобы обрезать градиенты во время обратного распространения, чтобы они никогда не превышали некоторый порог. Это называется Gradient Clipping. Этот метод чаще всего используется в рекуррентных нейронных сетях, так как пакетная нормализация сложно использовать в RNN, как мы увидим позже. Для других типов сетей обычно достаточно BN.\n",
    "В Keras реализация Gradient Clipping - это просто вопрос установки аргумента clipvalue или clipnorm при создании оптимизатора, например:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZYuLoYV9qIFL"
   },
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fjtdKYrJqIFL"
   },
   "source": [
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nzk_bHBkqIFL"
   },
   "source": [
    "Этот оптимизатор будет обрезать каждый компонент вектора градиента до значения от –1,0 до 1,0. Это означает, что все частные производные потери (в отношении каждого обучаемого параметра) будут ограничены между –1,0 и 1,0. Порог - это гиперпараметр, который вы можете настроить. Обратите внимание, что это может изменить ориентацию вектора градиента. Например, если исходный вектор градиента равен [0,9, 100,0], он в основном указывает в направлении второй оси; но как только вы обрезаете его по значению, вы получите [0,9, 1,0], который примерно указывает на диагональ между двумя осями. На практике этот подход работает хорошо. Если вы хотите убедиться, что Gradient Clipping не меняет направление вектора градиента, вы должны обрезать по норме, установив clipnorm вместо clipvalue. Это будет обрезать весь градиент, если его ℓ 2 нормы больше порога вы выбрали. Например, если вы установите clipnorm = 1.0 , то вектор [0.9, 100.0] будет обрезан до [0.00899964, 0.9999595], сохраняя его ориентацию, но почти исключая первый компонент. Если вы заметили, что градиенты сильно возрастают во время обучения (вы можете отслеживать размер градиентов с помощью TensorBoard), вы можете попробовать как урезать по значению, так и урезать по норме, с различными пороговыми значениями, и посмотрите, какая опция лучше всего работает на наборе при проверки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8BFHlI4qIFL"
   },
   "source": [
    "## Повторное использование предварительно обученных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e48bf-nIqIFL"
   },
   "source": [
    "Как правило, не очень хорошая идея тренировать очень большое DNN с нуля: вместо этого вы всегда должны пытаться найти существующую нейронную сеть, которая выполняет задачу, аналогичную той, которую вы пытаетесь решить, затем повторно используйте нижние уровни этой сети. Эта техника называется трансферным обучением. Это не только значительно ускорит обучение, но и потребует значительно меньшего количества тренировочных данных.\n",
    "Предположим, у вас есть доступ к DNN, который был обучен для классификации изображений на 100 различных категорий, включая животных, растения, транспортные средства и предметы быта. Теперь вы хотите обучить DNN для классификации определенных типов транспортных средств. Эти задачи очень похожи, даже частично перекрываются, поэтому вы должны попытаться повторно использовать части первой сети (см. Рис. 11-4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS8PXkI1qIFL"
   },
   "source": [
    "<img src = \"transfer.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DOWaQ37qIFL"
   },
   "source": [
    "Если входные изображения вашей новой задачи не имеют такой же размер, как те, что использовались в исходной задаче, вам обычно нужно добавить шаг предварительной обработки, чтобы изменить их размер до размера, ожидаемого исходной моделью. В более общем смысле, трансферное обучение будет работать лучше всего, когда входы имеют схожие низкоуровневые функции.\n",
    "Выходной слой исходной модели обычно следует заменять, поскольку он, скорее всего, вообще не полезен для новой задачи и может даже не иметь нужного количества выходов для новой задачи.\n",
    "Точно так же верхние скрытые слои исходной модели с меньшей вероятностью будут столь же полезны, как и нижние слои, поскольку высокоуровневые функции, наиболее полезные для новой задачи, могут значительно отличаться от тех, которые были наиболее полезны для исходной задачи. , Вы хотите найти правильное количество слоев для повторного использования.\n",
    "Примечание\n",
    "Чем больше похожих задач, тем больше слоев вы хотите использовать повторно (начиная с нижних уровней). Для очень похожих задач попробуйте сохранить все скрытые слои и просто заменить выходной слой.\n",
    "Попробуйте сначала заморозить все повторно используемые слои (т. е. Сделать их веса необучаемыми, чтобы градиентный спуск не изменил их), затем обучите свою модель и посмотрите, как она работает. Затем попробуйте разморозить один или два верхних скрытых слоя, чтобы позволить обратному распространению настроить их, и посмотреть, улучшится ли производительность. Чем больше у вас тренировочных данных, тем больше слоев вы сможете разморозить. Также полезно уменьшить скорость обучения, когда вы размораживаете повторно используемые слои: это позволит избежать разрушения их точно настроенных весов.\n",
    "Если вы все еще не можете добиться хорошей производительности, и у вас мало тренировочных данных, попробуйте сбросить верхний скрытый слой (слои) и снова заморозить все оставшиеся скрытые слои. Вы можете перебирать, пока не найдете нужное количество слоев для повторного использования. Если у вас много тренировочных данных, вы можете попробовать заменить верхние скрытые слои вместо того, чтобы отбрасывать их, и даже добавить больше скрытых слоев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yn33gWhoqIFV"
   },
   "source": [
    "# Как избежать переобучения через регуляризацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMheCNJxqIFV"
   },
   "source": [
    "Глубокие нейронные сети обычно имеют десятки тысяч параметров, иногда даже миллионы. Это дает им невероятную свободу и означает, что они могут вместить огромное количество сложных наборов данных. Но эта большая гибкость также делает сеть склонной к переобучению тренировочного набора. Нам нужна регуляризация.\n",
    "Мы уже реализовали один из лучших методов регуляризации: ранняя остановка. Более того, хотя пакетная нормализация была разработана для решения проблем с нестабильными градиентами, она также действует как довольно хороший регуляризатор. Далее мы рассмотрим другие популярные методы регуляризации для нейронных сетей: регуляризация ℓ 1 и out 2 , dropout и регуляризация с максимальной нормой.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MReoltJoqIFV"
   },
   "source": [
    "## $\\ell_1$ и $\\ell_2$ регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEVAQfVkqIFV"
   },
   "source": [
    "Ранее для простых линейных моделей, вы можете использовать регуляризацию ℓ 2 для ограничения весов соединений нейронной сети и / или регуляризацию ℓ 1, если вы хотите разреженную модель (со многими весами, равными 0). Вот как применить регуляризацию ℓ 2 к весам соединений слоя Keras, используя коэффициент регуляризации 0,01:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7FFATyo7qIFV"
   },
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "# or l1(0.1) for ℓ1 regularization with a factor or 0.1\n",
    "# or l1_l2(0.1, 0.01) for both ℓ1 and ℓ2 regularization, with factors 0.1 and 0.01 respectively"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6lLHnTNqIFV"
   },
   "source": [
    "Функция l2() возвращает регуляризатор, который будет вызываться на каждом шаге во время обучения для вычисления потери регуляризации. Это затем добавляется к окончательной потере. Как и следовало ожидать, вы можете просто использовать keras.regularizers.l1 (), если вы хотите regular 1 регуляризацию; если вы хотите регуляризацию ℓ 1 и ℓ 2 , используйте keras.regularizers.l1_l2 () (указав оба фактора регуляризации).\n",
    "Поскольку вы обычно хотите применить один и тот же регуляризатор ко всем слоям в вашей сети, а также использовать одну и ту же функцию активации и одну и ту же стратегию инициализации во всех скрытых слоях, вы можете обнаружить, что повторяете одни и те же аргументы. Это делает код уродливым и подверженным ошибкам. Чтобы избежать этого, вы можете попробовать рефакторинг вашего кода для использования циклов. Другой вариант - использовать функцию Python functools.partial () , которая позволяет создать тонкую оболочку для любого вызываемого объекта с некоторыми значениями аргументов по умолчанию:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g6BkA_hNqIFV"
   },
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(100, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rqz-9ANJqIFV"
   },
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7CYl2SsqIFV"
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKq3tPtHqIFV"
   },
   "source": [
    "Dropout является одним из самых популярных методов регуляризации для глубоких нейронных сетей. Он был proposed in a paper Джеффри Хинтоном в 2012 году и более подробно описан в 2014 paper Нитиша Шриваставы и др., И он оказался весьма успешным: даже современные нейронные сети получают  повышение точности на 2%, просто добавив Dropout. Это может показаться не так много, но когда модель уже имеет точность 95%, повышение точности на 2% означает снижение частоты ошибок почти на 40% (с 5% ошибки до примерно 3%).\n",
    "Это довольно простой алгоритм: на каждом этапе обучения, каждый нейрон ( в том числе входных нейронов, но всегда за исключением выходных нейронов) имеет вероятность р быть временно «обнуление» означает , что он будет полностью игнорироваться при этом шаге обучения, но он может быть активен на следующем шаге (см. Рисунок 11-9  ). Гиперпараметр p называется частотой выпадения , и он обычно устанавливается между 10% и 50%: ближе к 20–30% в повторяющихся нейронных сетях и ближе к 40–50% в сверточных нейронных сетях. После тренировки нейроны больше обнуляются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9iUatNNqIFV"
   },
   "source": [
    "<img src = \"dropout.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8sPbiAqtqIFV"
   },
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbiQV7RCqIFV"
   },
   "source": [
    "Сначала удивительно, что эта разрушительная техника работает вообще. Будет ли компания работать лучше, если ее сотрудникам будет приказывать бросать монеты каждое утро, чтобы решить, идти на работу или нет? Ну кто знает; возможно это будет! Компания будет вынуждена адаптировать свою организацию; он не мог рассчитывать на то, что какой-либо человек будет работать с кофемашиной или выполнять какие-либо другие важные задачи, поэтому этот опыт придется распространить на несколько человек. Сотрудникам придется учиться сотрудничать со многими из своих сотрудников, а не только с несколькими из них. Компания станет намного более устойчивой. Если один человек уйдет, это не будет иметь большого значения. Неясно, будет ли эта идея работать на самом деле для компаний, но это, безусловно, работает для нейронных сетей. Нейроны, обученные с отсева, не могут совместно адаптироваться с соседними нейронами; они должны быть максимально полезными сами по себе. Они также не могут чрезмерно полагаться только на несколько входных нейронов; они должны обратить внимание на каждый из своих входных нейронов. Они оказываются менее чувствительными к небольшим изменениям во входных данных. В итоге вы получаете более надежную сеть, которая лучше обобщается.\n",
    "Другой способ понять силу отсева - понять, что на каждом этапе обучения создается уникальная нейронная сеть. Поскольку каждый нейрон может присутствовать или отсутствовать, существует в общей сложности 2 N возможных сетей (где N - общее количество сбрасываемых нейронов). Это настолько большое число, что для одной и той же нейронной сети практически невозможно сделать выборку дважды. После того, как вы выполнили 10 000 этапов обучения, вы, по сути, обучили 10 000 различных нейронных сетей (каждая из которых имеет только один экземпляр обучения). Эти нейронные сети, очевидно, не являются независимыми, потому что они разделяют многие из своих весов, но, тем не менее, все они разные. Результирующая нейронная сеть может рассматриваться как ансамбль усреднения всех этих меньших нейронных сетей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0dhah9NqIFV"
   },
   "source": [
    "На практике вы можете обычно применять Dropout только к нейронам в верхнем 1-3 слоях (исключая выходной слой).\n",
    "Есть одна маленькая, но важная техническая деталь. Предположим, что p = 50%, и в этом случае во время тестирования нейрон будет подключен к вдвое большему количеству входных нейронов, чем было бы (в среднем) во время обучения. Чтобы компенсировать этот факт, нам нужно умножить веса входных соединений каждого нейрона на 0,5 после тренировки. Если мы этого не сделаем, каждый нейрон получит общий входной сигнал примерно в два раза больше, чем то, на котором обучалась сеть, и вряд ли будет работать хорошо. В более общем случае нам нужно умножить вес каждого входного соединения на вероятность удержания (1 - p ) после тренировки. В качестве альтернативы, мы можем разделить выход каждого нейрона на вероятность удержания во время тренировки (эти альтернативы не совсем эквивалентны, но они работают одинаково хорошо).\n",
    "Чтобы реализовать Dropout с использованием Keras , вы можете использовать слой `keras.layers.Dropout`. Во время обучения он случайным образом сбрасывает некоторые входные данные (устанавливая их в 0) и делит оставшиеся входные данные на вероятность удержания. После тренировки он вообще ничего не делает; он просто передает входные данные следующему слою. Следующий код применяет регуляризацию отсева перед каждым плотным слоем, используя коэффициент отсева 0,2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f1WTAceqIFV"
   },
   "source": [
    "Поскольку dropout активен только во время обучения, сравнение потери при обучении и потери при проверке может вводить в заблуждение. В частности, модель может соответствовать учебному набору и при этом иметь аналогичные потери при обучении и проверке. Поэтому обязательно оцените потерю тренировки без отсева (например, после тренировки).\n",
    "Если вы заметили, что модель переобучена, вы можете увеличить коэффициент dropout. И наоборот, вы должны попытаться уменьшить коэффициент dropout, если модель не соответствует тренировочному набору. Это также может помочь увеличить частоту выпадения для больших слоев и уменьшить ее для маленьких. Более того, многие современные архитектуры используют dropout только после последнего скрытого слоя, поэтому вы можете попробовать это, если полное выпадение слишком сильное.\n",
    "Выпадение имеет тенденцию значительно замедлять конвергенцию, но обычно это приводит к гораздо лучшей модели при правильной настройке. Таким образом, это, как правило, стоит дополнительного времени и усилий.\n",
    "Примечание\n",
    "Если вы хотите упорядочить самонормализующуюся сеть на основе функции активации SELU (как обсуждалось ранее), вы должны использовать альфа-отсев : это вариант отсева, который сохраняет среднее значение и стандартное отклонение ее входов (он был введен в та же бумага, что и SELU, так как регулярный dropout нарушит самонормализацию).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSG2T6uyqIFW"
   },
   "source": [
    "## Alpha Dropout"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YWKAuivFqIFW"
   },
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ts1fDgIlqIFW"
   },
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FEobPVn0qIFW"
   },
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fRqEgbMKqIFW"
   },
   "source": [
    "model.evaluate(X_train_scaled, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K-qqhz2vqIFW"
   },
   "source": [
    "history = model.fit(X_train_scaled, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsWOgDYmqIFW"
   },
   "source": [
    "## MC Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRWS5z00qIFW"
   },
   "source": [
    "В 2016 году в статье Ярина Гала и Зубина Гахрамани добавлено еще несколько веских причин для использования отсева:\n",
    "*\tВо-первых, в документе была установлена глубокая связь между сетями dropout (то есть нейронными сетями, содержащими слой dropout перед каждым весовым слоем) и приблизительным байесовским выводом, что дает dropout солидное математическое обоснование.\n",
    "*\tВо-вторых, авторы представили мощную технику, называемую MC Dropout , которая может повысить производительность любой обученной модели dropout без необходимости ее переподготовки или даже вообще ее изменять, обеспечивает гораздо лучший показатель неопределенности модели, а также удивительно прост для воплощать в жизнь.\n",
    "Если все это звучит как реклама «один странный трюк», взгляните на следующий код. Это полная реализация MC Dropout , улучшающая модель dropout, которую мы обучали ранее, без ее переподготовки:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QXBh6rVsqIFW"
   },
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qPxZqRCLqIFW"
   },
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mjIru3zqIFW"
   },
   "source": [
    "Мы просто делаем 100 прогнозов по тестовому набору, устанавливая training = True, чтобы гарантировать, что слой Dropout активен, и собираем прогнозы. Поскольку  Dropout активен, все прогнозы будут разными. Напомним, что predict() возвращает матрицу с одной строкой на экземпляр и одним столбцом на класс Поскольку в тестовом наборе 10 000 экземпляров и 10 классов, это матрица формы [10000, 10]. Мы складываем 100 таких матриц, поэтому y_probas - это массив формы [100, 10000, 10]. Как только мы усредним по первому измерению ( axis = 0 ), мы получим y_proba , массив формы [10000, 10], как мы получили бы с одним прогнозом. Вот и все! Усреднение по нескольким прогнозам с включенным отсевом дает нам оценку по методу Монте-Карло, которая, как правило, более надежна, чем результат одного прогноза с отсечкой. Например, давайте посмотрим на прогноз модели для первого экземпляра в наборе тестов Fashion MNIST, с отключением:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZqDlZli4qIFW"
   },
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeLZhr2QqIFW"
   },
   "source": [
    "Модель кажется почти уверенной, что этот образ относится к 9 классу (лыжный ботинок). Стоит ли доверять этому? Неужели так мало места для сомнений? Сравните это с прогнозами, сделанными при активации отсева:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SbcHMoBFqIFW"
   },
   "source": [
    "np.round(y_probas[:, :1], 2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nvjEPVYqIFW"
   },
   "source": [
    "Это рассказывает совсем другую историю: по-видимому, когда мы активируем dropout, модель больше не уверена. Кажется, он все еще предпочитает класс 9, но иногда он колеблется с классами 5 (сандали) и 7 (кроссовки), что имеет смысл, учитывая, что все они - обувь. После усреднения по первому измерению мы получаем следующие прогнозы MC Dropout:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ra-2_xP6qIFW"
   },
   "source": [
    "np.round(y_proba[:1], 2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUsamR_tqIFW"
   },
   "source": [
    "Модель все еще думает, что это изображение относится к 9 классу, но только с уверенностью 62%, что кажется гораздо более разумным, чем 99%. Кроме того, полезно точно знать, какие другие классы он считает вероятными. И вы также можете взглянуть на standard deviation of the probability estimates:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wK0IzNYWqIFW"
   },
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yj8iY9YPqIFW"
   },
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Hg61iHqqIFX"
   },
   "source": [
    "Очевидно, что в оценках вероятности есть довольно много отклонений: если вы строите систему, чувствительную к риску (например, медицинскую или финансовую систему), вам, вероятно, следует относиться к такому неопределенному прогнозу с особой осторожностью. Вы определенно не относитесь к этому как к 99% уверенному прогнозу. Более того, точность модели немного увеличилась с 86,8 до 86,9:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b6bEg-jjqIFX"
   },
   "source": [
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YY-_j-1tqIFX"
   },
   "source": [
    "Количество используемых образцов Монте-Карло (в данном примере 100) - это гиперпараметр, который вы можете настроить. Чем оно выше, тем точнее будут прогнозы и их оценки неопределенности. Однако, если вы удвоите его, время вывода также будет удвоено. Более того, выше определенного количества образцов вы заметите небольшое улучшение. Таким образом, ваша задача - найти правильный компромисс между задержкой и точностью, в зависимости от вашего приложения.\n",
    "Если ваша модель содержит другие слои, которые ведут себя особым образом во время обучения (например, слои BatchNormalization ), вам не следует форсировать режим обучения, как мы только что сделали. Вместо этого вы должны заменить слои Dropout следующим классом MCDropout :\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eMl17PySqIFX"
   },
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6Z0StOTqIFX"
   },
   "source": [
    "Здесь мы просто создаем подкласс Dropout- слоя и переопределяем метод call(), чтобы принудительно установить его training аргумент в True. Кроме того , вы можете определить MCAlphaDropout класс по подклассов AlphaDropout вместо этого. Если вы создаете модель с нуля, это просто вопрос использования MCDropout, а не Dropout . Но если у вас есть модель, которая уже была обучена с использованием Dropout , вам нужно создать новую модель, идентичную существующей модели, за исключением того, что она заменяет слои Dropout на MCDropout , а затем скопируйте веса существующей модели в вашу новую модель.\n",
    "Короче говоря, MC Dropout - это фантастическая методика, которая улучшает модели dropout и обеспечивает более точные оценки неопределенности. И, конечно же, поскольку это просто регулярное прекращение обучения во время тренировок, оно также действует как регуляризатор."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yLA1aVE6qIFX"
   },
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J4KA6ET-qIFX"
   },
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D4mk_1IUqIFX"
   },
   "source": [
    "mc_model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M6oLlbsqqIFX"
   },
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mTKcNraFqIFX"
   },
   "source": [
    "mc_model.set_weights(model.get_weights())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2cQ402-qIFX"
   },
   "source": [
    "Now we can use the model with MC Dropout:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FoqguzyWqIFX"
   },
   "source": [
    "np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQTvwwrAqIFX"
   },
   "source": [
    "## Регуляризация Max-Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofc6gl9sqIFX"
   },
   "source": [
    "Другой метод регуляризации, который популярен для нейронных сетей, называется регуляризацией по максимальной норме : для каждого нейрона он ограничивает веса w входящих соединений так, что ∥ w ∥ 2 ≤ r , где r - гиперпараметр максимальной нормы, а ∥ · ∥ 2 является нормой ℓ 2 .  \n",
    "Макс-норма регуляризации не добавляет термин потери регуляризации к общей функции потерь. Вместо этого он обычно реализуется путем вычисления ∥ w ∥ 2 после каждого шага обучения и при необходимости масштабирования w ( w ← w r / / w ‖ 2 ).\n",
    "Снижение r увеличивает степень регуляризации и помогает уменьшить переобучение. Регуляризация макс-нормы также может помочь решить проблемы нестабильных градиентов (если вы не используете пакетную нормализацию).\n",
    "Чтобы реализовать регуляризацию max-norm в Keras, установите для аргумента kernel_constraint каждого скрытого слоя ограничение max_norm() с соответствующим значением max, например:\n",
    "```python\n",
    "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                   kernel_constraint=keras.constraints.max_norm(1.))\n",
    "```\n",
    "После каждой обучающей итерации метод fit() модели будет вызывать объект, возвращаемый max_norm(), передавая ему веса слоя и получая взамен измененные веса, которые затем заменяют веса слоя. Как вы увидите плзднее, вы можете при необходимости определить свою собственную функцию ограничения и использовать ее как `kernel_constraint`. Вы также можете ограничить условия смещения, установив аргумент `bias_constraint`.\n",
    "Функция max_norm() имеет аргумент оси, который по умолчанию равен 0 . Плотный слой обычно имеет вес формы [number of inputs, number of neurons], поэтому использование axis = 0 означает , что максимальное ограничение-норма будет применяться независимо друг от друга , чтобы вектор весовых коэффициентов каждого нейрона. Если вы хотите использовать max-norm со сверточными слоями, убедитесь, что вы правильно установили аргумент оси ограничения max_norm() (обычно axis = [0, 1, 2] )."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GDdhgKqgqIFX"
   },
   "source": [
    "layer = keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                           kernel_constraint=keras.constraints.max_norm(1.))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0_v2z1gEqIFX"
   },
   "source": [
    "MaxNormDense = partial(keras.layers.Dense,\n",
    "                       activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    MaxNormDense(300),\n",
    "    MaxNormDense(100),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7rBW7AN2qIFX"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GlnIbzJTqIFY"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nav_menu": {
   "height": "360px",
   "width": "416px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
